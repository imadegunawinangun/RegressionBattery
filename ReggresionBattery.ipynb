{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReggresionBattery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxZfSFnbhCtP9T13++Rvou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imadegunawinangun/RegressionBattery/blob/main/ReggresionBattery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0i260XWboDz",
        "outputId": "5a1c5a77-97ca-4b3b-e82f-c904a17c4ffb"
      },
      "source": [
        "!git clone https://github.com/imadegunawinangun/RegressionBattery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RegressionBattery'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1IMfA5w1ruT",
        "outputId": "57920430-7a4e-40bc-ea83-6e06909c0954"
      },
      "source": [
        "!ls RegressionBattery  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN_INPUT.csv  NN_INPUT.xlsx  NN_OUTPUT.csv  NN_OUTPUT.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxwccNYy12fE"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import ntpath\n",
        "import random\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_-TkOGkqw5x",
        "outputId": "74329811-3193-4584-cf19-81ce92bac60b"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.8MB/s \n",
            "\u001b[?25h  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjI6rbf17pDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b44b456-98ff-4417-dac7-2f7a0dcdfb14"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import kerastuner.tuners as kt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "BCE135xb2DJV",
        "outputId": "7c68d27e-0471-4c02-92ea-cff7b827f977"
      },
      "source": [
        "datadir = 'RegressionBattery/'\n",
        "df=pd.read_excel(os.path.join(datadir, 'NN_INPUT.xlsx'),header=None)\n",
        "label = pd.read_excel(os.path.join(datadir, 'NN_OUTPUT.xlsx'),header=None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "df[601]=label\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "      <th>590</th>\n",
              "      <th>591</th>\n",
              "      <th>592</th>\n",
              "      <th>593</th>\n",
              "      <th>594</th>\n",
              "      <th>595</th>\n",
              "      <th>596</th>\n",
              "      <th>597</th>\n",
              "      <th>598</th>\n",
              "      <th>599</th>\n",
              "      <th>601</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.30000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.00982</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.02738</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.29000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.00982</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.02738</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.28000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.00982</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.02738</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.27000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.00982</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.02738</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.00982</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.02738</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>0.01010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>0.01009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>0.01005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>0.01001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>576 rows × 601 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3  ...       597      598       599  601\n",
              "0    0.30000  0.073561  0.147409  0.210614  ...  0.027025  0.02738  0.026312  0.1\n",
              "1    0.29000  0.073561  0.147409  0.210614  ...  0.027025  0.02738  0.026312  0.1\n",
              "2    0.28000  0.073561  0.147409  0.210614  ...  0.027025  0.02738  0.026312  0.1\n",
              "3    0.27000  0.073561  0.147409  0.210614  ...  0.027025  0.02738  0.026312  0.1\n",
              "4    0.26000  0.073561  0.147409  0.210614  ...  0.027025  0.02738  0.026312  0.1\n",
              "..       ...       ...       ...       ...  ...       ...      ...       ...  ...\n",
              "571  0.01010  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  1.0\n",
              "572  0.01009  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  1.0\n",
              "573  0.01005  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  1.0\n",
              "574  0.01001  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  1.0\n",
              "575  0.01000  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  1.0\n",
              "\n",
              "[576 rows x 601 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "UakR5eDM2ipm",
        "outputId": "f447962b-8c6f-45ab-9d53-6581437aee2e"
      },
      "source": [
        "num_bins = 10\n",
        "hist, bins = np.histogram(label[0], num_bins)\n",
        "center = (bins[:-1]+ bins[1:]) * 0.5\n",
        "plt.bar(center, hist, width=0.05)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSuar0pA6VY7"
      },
      "source": [
        "train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "aKrRkQ-s6gAh",
        "outputId": "dd5292b1-440c-4fb4-f5eb-3ead0180ba1b"
      },
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.116511</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>0.275200</td>\n",
              "      <td>0.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.058854</td>\n",
              "      <td>0.071671</td>\n",
              "      <td>-0.140519</td>\n",
              "      <td>0.014994</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.115707</td>\n",
              "      <td>0.144951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.085517</td>\n",
              "      <td>0.096743</td>\n",
              "      <td>-0.185782</td>\n",
              "      <td>0.023977</td>\n",
              "      <td>0.127377</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.215287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.100886</td>\n",
              "      <td>0.121298</td>\n",
              "      <td>-0.204950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.118720</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.274317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.114036</td>\n",
              "      <td>0.134918</td>\n",
              "      <td>-0.203877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.119391</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.302397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.069915</td>\n",
              "      <td>0.181603</td>\n",
              "      <td>-0.083078</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.456336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.076755</td>\n",
              "      <td>0.177314</td>\n",
              "      <td>-0.030682</td>\n",
              "      <td>-0.011194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.458834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.078806</td>\n",
              "      <td>0.175120</td>\n",
              "      <td>-0.014412</td>\n",
              "      <td>-0.012480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027380</td>\n",
              "      <td>0.456707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.076142</td>\n",
              "      <td>0.172966</td>\n",
              "      <td>-0.031189</td>\n",
              "      <td>-0.004879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026312</td>\n",
              "      <td>0.448847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>461.0</td>\n",
              "      <td>0.525380</td>\n",
              "      <td>0.290101</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>601 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     count      mean       std  ...       50%       75%       max\n",
              "0    461.0  0.176745  0.116511  ...  0.224000  0.275200  0.330000\n",
              "1    461.0  0.058854  0.071671  ...  0.073561  0.115707  0.144951\n",
              "2    461.0  0.085517  0.096743  ...  0.127377  0.147409  0.215287\n",
              "3    461.0  0.100886  0.121298  ...  0.118720  0.210614  0.274317\n",
              "4    461.0  0.114036  0.134918  ...  0.119391  0.264647  0.302397\n",
              "..     ...       ...       ...  ...       ...       ...       ...\n",
              "596  461.0  0.069915  0.181603  ...  0.000000  0.025631  0.456336\n",
              "597  461.0  0.076755  0.177314  ...  0.000000  0.027025  0.458834\n",
              "598  461.0  0.078806  0.175120  ...  0.000000  0.027380  0.456707\n",
              "599  461.0  0.076142  0.172966  ...  0.000000  0.026312  0.448847\n",
              "601  461.0  0.525380  0.290101  ...  0.500000  0.800000  1.000000\n",
              "\n",
              "[601 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t45X8WWx6t3f"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop(601)\n",
        "test_labels = test_features.pop(601)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "N8al9UqX7H-J",
        "outputId": "6a2f8423-2765-4fe5-914a-59bb91588e7e"
      },
      "source": [
        "train_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "      <th>590</th>\n",
              "      <th>591</th>\n",
              "      <th>592</th>\n",
              "      <th>593</th>\n",
              "      <th>594</th>\n",
              "      <th>595</th>\n",
              "      <th>596</th>\n",
              "      <th>597</th>\n",
              "      <th>598</th>\n",
              "      <th>599</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0.002018</td>\n",
              "      <td>0.115707</td>\n",
              "      <td>0.215287</td>\n",
              "      <td>0.274317</td>\n",
              "      <td>0.302397</td>\n",
              "      <td>0.303229</td>\n",
              "      <td>0.284524</td>\n",
              "      <td>0.255105</td>\n",
              "      <td>0.219821</td>\n",
              "      <td>0.183164</td>\n",
              "      <td>0.150725</td>\n",
              "      <td>0.122407</td>\n",
              "      <td>0.098441</td>\n",
              "      <td>0.079858</td>\n",
              "      <td>0.067125</td>\n",
              "      <td>0.060564</td>\n",
              "      <td>0.060399</td>\n",
              "      <td>0.065361</td>\n",
              "      <td>0.070203</td>\n",
              "      <td>0.073348</td>\n",
              "      <td>0.074923</td>\n",
              "      <td>0.074783</td>\n",
              "      <td>0.072894</td>\n",
              "      <td>0.069248</td>\n",
              "      <td>0.064192</td>\n",
              "      <td>0.060036</td>\n",
              "      <td>0.057317</td>\n",
              "      <td>0.056120</td>\n",
              "      <td>0.056429</td>\n",
              "      <td>0.058239</td>\n",
              "      <td>0.061585</td>\n",
              "      <td>0.066519</td>\n",
              "      <td>0.072970</td>\n",
              "      <td>0.079781</td>\n",
              "      <td>0.081434</td>\n",
              "      <td>0.076319</td>\n",
              "      <td>0.064104</td>\n",
              "      <td>0.044897</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.376290</td>\n",
              "      <td>0.377720</td>\n",
              "      <td>0.380096</td>\n",
              "      <td>0.383502</td>\n",
              "      <td>0.388338</td>\n",
              "      <td>0.394754</td>\n",
              "      <td>0.402813</td>\n",
              "      <td>0.412581</td>\n",
              "      <td>0.424832</td>\n",
              "      <td>0.439716</td>\n",
              "      <td>0.456766</td>\n",
              "      <td>0.475089</td>\n",
              "      <td>0.490726</td>\n",
              "      <td>0.502487</td>\n",
              "      <td>0.510247</td>\n",
              "      <td>0.513517</td>\n",
              "      <td>0.511897</td>\n",
              "      <td>0.506290</td>\n",
              "      <td>0.499826</td>\n",
              "      <td>0.493372</td>\n",
              "      <td>0.486973</td>\n",
              "      <td>0.480670</td>\n",
              "      <td>0.474502</td>\n",
              "      <td>0.468508</td>\n",
              "      <td>0.462724</td>\n",
              "      <td>0.457186</td>\n",
              "      <td>0.451930</td>\n",
              "      <td>0.446988</td>\n",
              "      <td>0.442596</td>\n",
              "      <td>0.439608</td>\n",
              "      <td>0.438457</td>\n",
              "      <td>0.438863</td>\n",
              "      <td>0.441045</td>\n",
              "      <td>0.444901</td>\n",
              "      <td>0.448922</td>\n",
              "      <td>0.452739</td>\n",
              "      <td>0.456336</td>\n",
              "      <td>0.458834</td>\n",
              "      <td>0.456707</td>\n",
              "      <td>0.448847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.026155</td>\n",
              "      <td>0.054080</td>\n",
              "      <td>0.071790</td>\n",
              "      <td>0.078588</td>\n",
              "      <td>0.072685</td>\n",
              "      <td>0.055741</td>\n",
              "      <td>0.039670</td>\n",
              "      <td>0.028421</td>\n",
              "      <td>0.021215</td>\n",
              "      <td>0.020979</td>\n",
              "      <td>0.023061</td>\n",
              "      <td>0.020533</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>0.007720</td>\n",
              "      <td>-0.003559</td>\n",
              "      <td>-0.013805</td>\n",
              "      <td>-0.016337</td>\n",
              "      <td>-0.012267</td>\n",
              "      <td>-0.000888</td>\n",
              "      <td>0.008191</td>\n",
              "      <td>0.014581</td>\n",
              "      <td>0.026881</td>\n",
              "      <td>0.040758</td>\n",
              "      <td>0.055279</td>\n",
              "      <td>0.071355</td>\n",
              "      <td>0.085519</td>\n",
              "      <td>0.097187</td>\n",
              "      <td>0.103830</td>\n",
              "      <td>0.103570</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>0.081523</td>\n",
              "      <td>0.065263</td>\n",
              "      <td>0.049566</td>\n",
              "      <td>0.035292</td>\n",
              "      <td>0.023913</td>\n",
              "      <td>0.019220</td>\n",
              "      <td>0.017261</td>\n",
              "      <td>0.018643</td>\n",
              "      <td>0.023282</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710498</td>\n",
              "      <td>0.656102</td>\n",
              "      <td>0.593100</td>\n",
              "      <td>0.530620</td>\n",
              "      <td>0.471463</td>\n",
              "      <td>0.416999</td>\n",
              "      <td>0.371694</td>\n",
              "      <td>0.338804</td>\n",
              "      <td>0.320338</td>\n",
              "      <td>0.317153</td>\n",
              "      <td>0.329724</td>\n",
              "      <td>0.353596</td>\n",
              "      <td>0.370434</td>\n",
              "      <td>0.375470</td>\n",
              "      <td>0.368291</td>\n",
              "      <td>0.348311</td>\n",
              "      <td>0.317968</td>\n",
              "      <td>0.289482</td>\n",
              "      <td>0.266559</td>\n",
              "      <td>0.249800</td>\n",
              "      <td>0.239058</td>\n",
              "      <td>0.234101</td>\n",
              "      <td>0.233989</td>\n",
              "      <td>0.238508</td>\n",
              "      <td>0.247795</td>\n",
              "      <td>0.261904</td>\n",
              "      <td>0.273886</td>\n",
              "      <td>0.253327</td>\n",
              "      <td>0.193049</td>\n",
              "      <td>0.092955</td>\n",
              "      <td>-0.022010</td>\n",
              "      <td>-0.139839</td>\n",
              "      <td>-0.217748</td>\n",
              "      <td>-0.244727</td>\n",
              "      <td>-0.223728</td>\n",
              "      <td>-0.162931</td>\n",
              "      <td>-0.083078</td>\n",
              "      <td>-0.030682</td>\n",
              "      <td>-0.014412</td>\n",
              "      <td>-0.031189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.014994</td>\n",
              "      <td>0.023977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018866</td>\n",
              "      <td>0.189229</td>\n",
              "      <td>0.429128</td>\n",
              "      <td>0.544688</td>\n",
              "      <td>0.455607</td>\n",
              "      <td>0.312028</td>\n",
              "      <td>0.216280</td>\n",
              "      <td>0.209510</td>\n",
              "      <td>0.288117</td>\n",
              "      <td>0.366263</td>\n",
              "      <td>0.414209</td>\n",
              "      <td>0.422164</td>\n",
              "      <td>0.382744</td>\n",
              "      <td>0.296729</td>\n",
              "      <td>0.172798</td>\n",
              "      <td>0.024420</td>\n",
              "      <td>-0.095546</td>\n",
              "      <td>-0.197441</td>\n",
              "      <td>-0.219877</td>\n",
              "      <td>-0.181150</td>\n",
              "      <td>-0.117865</td>\n",
              "      <td>-0.071429</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137538</td>\n",
              "      <td>0.087230</td>\n",
              "      <td>0.056292</td>\n",
              "      <td>0.044881</td>\n",
              "      <td>0.051024</td>\n",
              "      <td>0.066724</td>\n",
              "      <td>0.090311</td>\n",
              "      <td>0.121932</td>\n",
              "      <td>0.162282</td>\n",
              "      <td>0.205663</td>\n",
              "      <td>0.222390</td>\n",
              "      <td>0.203419</td>\n",
              "      <td>0.145341</td>\n",
              "      <td>0.058407</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>-0.035917</td>\n",
              "      <td>-0.058254</td>\n",
              "      <td>-0.066449</td>\n",
              "      <td>-0.060842</td>\n",
              "      <td>-0.043221</td>\n",
              "      <td>-0.015417</td>\n",
              "      <td>0.022029</td>\n",
              "      <td>0.074997</td>\n",
              "      <td>0.070256</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>-0.093804</td>\n",
              "      <td>-0.181419</td>\n",
              "      <td>-0.190554</td>\n",
              "      <td>-0.143411</td>\n",
              "      <td>-0.085124</td>\n",
              "      <td>-0.035487</td>\n",
              "      <td>0.001602</td>\n",
              "      <td>0.014519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.140519</td>\n",
              "      <td>-0.185782</td>\n",
              "      <td>-0.204950</td>\n",
              "      <td>-0.203877</td>\n",
              "      <td>-0.188184</td>\n",
              "      <td>-0.163691</td>\n",
              "      <td>-0.137148</td>\n",
              "      <td>-0.110467</td>\n",
              "      <td>-0.084027</td>\n",
              "      <td>-0.060112</td>\n",
              "      <td>-0.040557</td>\n",
              "      <td>-0.027157</td>\n",
              "      <td>-0.018496</td>\n",
              "      <td>-0.013127</td>\n",
              "      <td>-0.010091</td>\n",
              "      <td>-0.006917</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>0.005746</td>\n",
              "      <td>0.010165</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.015158</td>\n",
              "      <td>0.017739</td>\n",
              "      <td>0.023636</td>\n",
              "      <td>0.017828</td>\n",
              "      <td>0.022374</td>\n",
              "      <td>0.032021</td>\n",
              "      <td>0.049437</td>\n",
              "      <td>0.072811</td>\n",
              "      <td>0.101758</td>\n",
              "      <td>0.134824</td>\n",
              "      <td>0.160644</td>\n",
              "      <td>0.165181</td>\n",
              "      <td>0.139159</td>\n",
              "      <td>0.084287</td>\n",
              "      <td>0.020668</td>\n",
              "      <td>-0.005408</td>\n",
              "      <td>-0.014474</td>\n",
              "      <td>-0.009847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>0.012090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>0.012070</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018493</td>\n",
              "      <td>0.108016</td>\n",
              "      <td>0.214906</td>\n",
              "      <td>0.290445</td>\n",
              "      <td>0.367015</td>\n",
              "      <td>0.484669</td>\n",
              "      <td>0.662650</td>\n",
              "      <td>0.800261</td>\n",
              "      <td>0.800329</td>\n",
              "      <td>0.649983</td>\n",
              "      <td>0.469062</td>\n",
              "      <td>0.321419</td>\n",
              "      <td>0.229845</td>\n",
              "      <td>0.197277</td>\n",
              "      <td>0.213491</td>\n",
              "      <td>0.235863</td>\n",
              "      <td>0.252949</td>\n",
              "      <td>0.264705</td>\n",
              "      <td>0.270252</td>\n",
              "      <td>0.268967</td>\n",
              "      <td>0.260519</td>\n",
              "      <td>0.246462</td>\n",
              "      <td>0.233575</td>\n",
              "      <td>0.224067</td>\n",
              "      <td>0.218305</td>\n",
              "      <td>0.215854</td>\n",
              "      <td>0.214005</td>\n",
              "      <td>0.212038</td>\n",
              "      <td>0.209947</td>\n",
              "      <td>0.206972</td>\n",
              "      <td>0.199999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>0.279900</td>\n",
              "      <td>0.144951</td>\n",
              "      <td>0.127377</td>\n",
              "      <td>0.118720</td>\n",
              "      <td>0.119391</td>\n",
              "      <td>0.129563</td>\n",
              "      <td>0.147371</td>\n",
              "      <td>0.164016</td>\n",
              "      <td>0.177264</td>\n",
              "      <td>0.186812</td>\n",
              "      <td>0.192337</td>\n",
              "      <td>0.193933</td>\n",
              "      <td>0.192606</td>\n",
              "      <td>0.188501</td>\n",
              "      <td>0.181453</td>\n",
              "      <td>0.171492</td>\n",
              "      <td>0.159974</td>\n",
              "      <td>0.152465</td>\n",
              "      <td>0.150634</td>\n",
              "      <td>0.154764</td>\n",
              "      <td>0.164984</td>\n",
              "      <td>0.178701</td>\n",
              "      <td>0.184282</td>\n",
              "      <td>0.177850</td>\n",
              "      <td>0.159178</td>\n",
              "      <td>0.127820</td>\n",
              "      <td>0.088775</td>\n",
              "      <td>0.062188</td>\n",
              "      <td>0.053230</td>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.087609</td>\n",
              "      <td>0.127782</td>\n",
              "      <td>0.168535</td>\n",
              "      <td>0.207179</td>\n",
              "      <td>0.242703</td>\n",
              "      <td>0.277054</td>\n",
              "      <td>0.308786</td>\n",
              "      <td>0.329630</td>\n",
              "      <td>0.336743</td>\n",
              "      <td>0.328315</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064659</td>\n",
              "      <td>0.131977</td>\n",
              "      <td>0.172698</td>\n",
              "      <td>0.185934</td>\n",
              "      <td>0.165439</td>\n",
              "      <td>0.092868</td>\n",
              "      <td>0.030759</td>\n",
              "      <td>0.045780</td>\n",
              "      <td>0.192428</td>\n",
              "      <td>0.017374</td>\n",
              "      <td>0.026137</td>\n",
              "      <td>0.053995</td>\n",
              "      <td>0.100311</td>\n",
              "      <td>0.157608</td>\n",
              "      <td>0.219729</td>\n",
              "      <td>0.268737</td>\n",
              "      <td>0.293587</td>\n",
              "      <td>0.302333</td>\n",
              "      <td>0.306670</td>\n",
              "      <td>0.309578</td>\n",
              "      <td>0.313679</td>\n",
              "      <td>0.319523</td>\n",
              "      <td>0.319618</td>\n",
              "      <td>0.311061</td>\n",
              "      <td>0.292988</td>\n",
              "      <td>0.264813</td>\n",
              "      <td>0.229601</td>\n",
              "      <td>0.197149</td>\n",
              "      <td>0.167934</td>\n",
              "      <td>0.142873</td>\n",
              "      <td>0.123085</td>\n",
              "      <td>0.107094</td>\n",
              "      <td>0.088557</td>\n",
              "      <td>0.065708</td>\n",
              "      <td>0.039597</td>\n",
              "      <td>0.019497</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.011194</td>\n",
              "      <td>-0.012480</td>\n",
              "      <td>-0.004879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.223000</td>\n",
              "      <td>0.073561</td>\n",
              "      <td>0.147409</td>\n",
              "      <td>0.210614</td>\n",
              "      <td>0.264647</td>\n",
              "      <td>0.300705</td>\n",
              "      <td>0.313644</td>\n",
              "      <td>0.303858</td>\n",
              "      <td>0.272082</td>\n",
              "      <td>0.218681</td>\n",
              "      <td>0.149838</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.014634</td>\n",
              "      <td>-0.021616</td>\n",
              "      <td>-0.048814</td>\n",
              "      <td>-0.065958</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>-0.060520</td>\n",
              "      <td>-0.039829</td>\n",
              "      <td>-0.013576</td>\n",
              "      <td>0.017381</td>\n",
              "      <td>0.083372</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.183037</td>\n",
              "      <td>0.172107</td>\n",
              "      <td>0.103393</td>\n",
              "      <td>0.015714</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>0.018067</td>\n",
              "      <td>0.120867</td>\n",
              "      <td>0.317826</td>\n",
              "      <td>0.513179</td>\n",
              "      <td>0.647989</td>\n",
              "      <td>0.641456</td>\n",
              "      <td>0.428311</td>\n",
              "      <td>0.060998</td>\n",
              "      <td>-0.132606</td>\n",
              "      <td>-0.217928</td>\n",
              "      <td>-0.221580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047917</td>\n",
              "      <td>0.050995</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.051989</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.042475</td>\n",
              "      <td>0.034971</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.008305</td>\n",
              "      <td>0.005969</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.028174</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.015104</td>\n",
              "      <td>0.013825</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.012930</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.013796</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.019863</td>\n",
              "      <td>0.021771</td>\n",
              "      <td>0.023714</td>\n",
              "      <td>0.025631</td>\n",
              "      <td>0.027025</td>\n",
              "      <td>0.027380</td>\n",
              "      <td>0.026312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.281000</td>\n",
              "      <td>0.026155</td>\n",
              "      <td>0.054080</td>\n",
              "      <td>0.071790</td>\n",
              "      <td>0.078588</td>\n",
              "      <td>0.072685</td>\n",
              "      <td>0.055741</td>\n",
              "      <td>0.039670</td>\n",
              "      <td>0.028421</td>\n",
              "      <td>0.021215</td>\n",
              "      <td>0.020979</td>\n",
              "      <td>0.023061</td>\n",
              "      <td>0.020533</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>0.007720</td>\n",
              "      <td>-0.003559</td>\n",
              "      <td>-0.013805</td>\n",
              "      <td>-0.016337</td>\n",
              "      <td>-0.012267</td>\n",
              "      <td>-0.000888</td>\n",
              "      <td>0.008191</td>\n",
              "      <td>0.014581</td>\n",
              "      <td>0.026881</td>\n",
              "      <td>0.040758</td>\n",
              "      <td>0.055279</td>\n",
              "      <td>0.071355</td>\n",
              "      <td>0.085519</td>\n",
              "      <td>0.097187</td>\n",
              "      <td>0.103830</td>\n",
              "      <td>0.103570</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>0.081523</td>\n",
              "      <td>0.065263</td>\n",
              "      <td>0.049566</td>\n",
              "      <td>0.035292</td>\n",
              "      <td>0.023913</td>\n",
              "      <td>0.019220</td>\n",
              "      <td>0.017261</td>\n",
              "      <td>0.018643</td>\n",
              "      <td>0.023282</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710498</td>\n",
              "      <td>0.656102</td>\n",
              "      <td>0.593100</td>\n",
              "      <td>0.530620</td>\n",
              "      <td>0.471463</td>\n",
              "      <td>0.416999</td>\n",
              "      <td>0.371694</td>\n",
              "      <td>0.338804</td>\n",
              "      <td>0.320338</td>\n",
              "      <td>0.317153</td>\n",
              "      <td>0.329724</td>\n",
              "      <td>0.353596</td>\n",
              "      <td>0.370434</td>\n",
              "      <td>0.375470</td>\n",
              "      <td>0.368291</td>\n",
              "      <td>0.348311</td>\n",
              "      <td>0.317968</td>\n",
              "      <td>0.289482</td>\n",
              "      <td>0.266559</td>\n",
              "      <td>0.249800</td>\n",
              "      <td>0.239058</td>\n",
              "      <td>0.234101</td>\n",
              "      <td>0.233989</td>\n",
              "      <td>0.238508</td>\n",
              "      <td>0.247795</td>\n",
              "      <td>0.261904</td>\n",
              "      <td>0.273886</td>\n",
              "      <td>0.253327</td>\n",
              "      <td>0.193049</td>\n",
              "      <td>0.092955</td>\n",
              "      <td>-0.022010</td>\n",
              "      <td>-0.139839</td>\n",
              "      <td>-0.217748</td>\n",
              "      <td>-0.244727</td>\n",
              "      <td>-0.223728</td>\n",
              "      <td>-0.162931</td>\n",
              "      <td>-0.083078</td>\n",
              "      <td>-0.030682</td>\n",
              "      <td>-0.014412</td>\n",
              "      <td>-0.031189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0.001300</td>\n",
              "      <td>-0.140519</td>\n",
              "      <td>-0.185782</td>\n",
              "      <td>-0.204950</td>\n",
              "      <td>-0.203877</td>\n",
              "      <td>-0.188184</td>\n",
              "      <td>-0.163691</td>\n",
              "      <td>-0.137148</td>\n",
              "      <td>-0.110467</td>\n",
              "      <td>-0.084027</td>\n",
              "      <td>-0.060112</td>\n",
              "      <td>-0.040557</td>\n",
              "      <td>-0.027157</td>\n",
              "      <td>-0.018496</td>\n",
              "      <td>-0.013127</td>\n",
              "      <td>-0.010091</td>\n",
              "      <td>-0.006917</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>0.005746</td>\n",
              "      <td>0.010165</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.015158</td>\n",
              "      <td>0.017739</td>\n",
              "      <td>0.023636</td>\n",
              "      <td>0.017828</td>\n",
              "      <td>0.022374</td>\n",
              "      <td>0.032021</td>\n",
              "      <td>0.049437</td>\n",
              "      <td>0.072811</td>\n",
              "      <td>0.101758</td>\n",
              "      <td>0.134824</td>\n",
              "      <td>0.160644</td>\n",
              "      <td>0.165181</td>\n",
              "      <td>0.139159</td>\n",
              "      <td>0.084287</td>\n",
              "      <td>0.020668</td>\n",
              "      <td>-0.005408</td>\n",
              "      <td>-0.014474</td>\n",
              "      <td>-0.009847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>461 rows × 600 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2  ...       597       598       599\n",
              "414  0.002018  0.115707  0.215287  ...  0.458834  0.456707  0.448847\n",
              "96   0.265000  0.026155  0.054080  ... -0.030682 -0.014412 -0.031189\n",
              "480  0.291000  0.014994  0.023977  ...  0.000000  0.000000  0.000000\n",
              "318  0.000200 -0.140519 -0.185782  ...  0.000000  0.000000  0.000000\n",
              "540  0.012090  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "542  0.012070  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "182  0.279900  0.144951  0.127377  ... -0.011194 -0.012480 -0.004879\n",
              "32   0.223000  0.073561  0.147409  ...  0.027025  0.027380  0.026312\n",
              "80   0.281000  0.026155  0.054080  ... -0.030682 -0.014412 -0.031189\n",
              "307  0.001300 -0.140519 -0.185782  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[461 rows x 600 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdbxAixC68DF",
        "outputId": "ca11464a-3083-4798-b06c-83af25a1794f"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414    1.0\n",
              "96     0.3\n",
              "480    0.8\n",
              "318    0.2\n",
              "540    0.6\n",
              "      ... \n",
              "542    0.7\n",
              "182    0.2\n",
              "32     0.4\n",
              "80     0.1\n",
              "307    0.2\n",
              "Name: 601, Length: 461, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6DFtdwBa62-3",
        "outputId": "cc2eea94-185c-4347-d614-0fc43305c18f"
      },
      "source": [
        "train_dataset.describe().transpose()[['mean', 'std']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.116511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058854</td>\n",
              "      <td>0.071671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085517</td>\n",
              "      <td>0.096743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.100886</td>\n",
              "      <td>0.121298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.114036</td>\n",
              "      <td>0.134918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>0.069915</td>\n",
              "      <td>0.181603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>0.076755</td>\n",
              "      <td>0.177314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>0.078806</td>\n",
              "      <td>0.175120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>0.076142</td>\n",
              "      <td>0.172966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>0.525380</td>\n",
              "      <td>0.290101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>601 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         mean       std\n",
              "0    0.176745  0.116511\n",
              "1    0.058854  0.071671\n",
              "2    0.085517  0.096743\n",
              "3    0.100886  0.121298\n",
              "4    0.114036  0.134918\n",
              "..        ...       ...\n",
              "596  0.069915  0.181603\n",
              "597  0.076755  0.177314\n",
              "598  0.078806  0.175120\n",
              "599  0.076142  0.172966\n",
              "601  0.525380  0.290101\n",
              "\n",
              "[601 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SstTLBfnNnzS"
      },
      "source": [
        "def nn_supercapacitor(hp):\n",
        "  model = keras.Sequential()\n",
        "  for i in range(hp.Int(\"Dense Layers\", min_value=1, max_value=10)):\n",
        "      model.add(keras.layers.Dense(hp.Choice(f\"Dense_{i}_layer\", [32,64,128,256,512,16,8]), \n",
        "                                   activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(hp.Choice(f\"learning_rate\", [1e-2, 1e-3, 1e-5, 1e-8])))\n",
        "  return model\n",
        "def nn_supercapacitor():\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(16, activation='relu'))\n",
        "  model.add(keras.layers.Dense(32, activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(1e-5))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL57ziZbAly8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef52c40-1933-42a1-f77d-eb09cd1a18cb"
      },
      "source": [
        "model = nn_supercapacitor()\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    validation_split=0.2,\n",
        "     epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "12/12 [==============================] - 1s 21ms/step - loss: 0.6404 - val_loss: 0.4662\n",
            "Epoch 2/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5729 - val_loss: 0.4167\n",
            "Epoch 3/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5459 - val_loss: 0.3797\n",
            "Epoch 4/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5156 - val_loss: 0.3559\n",
            "Epoch 5/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5003 - val_loss: 0.3335\n",
            "Epoch 6/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4190 - val_loss: 0.3150\n",
            "Epoch 7/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4255 - val_loss: 0.2968\n",
            "Epoch 8/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3803 - val_loss: 0.2805\n",
            "Epoch 9/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3837 - val_loss: 0.2631\n",
            "Epoch 10/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3487 - val_loss: 0.2460\n",
            "Epoch 11/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3361 - val_loss: 0.2285\n",
            "Epoch 12/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3386 - val_loss: 0.2130\n",
            "Epoch 13/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3004 - val_loss: 0.1966\n",
            "Epoch 14/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2889 - val_loss: 0.1806\n",
            "Epoch 15/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2784 - val_loss: 0.1672\n",
            "Epoch 16/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2422 - val_loss: 0.1548\n",
            "Epoch 17/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2433 - val_loss: 0.1439\n",
            "Epoch 18/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2199 - val_loss: 0.1344\n",
            "Epoch 19/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2140 - val_loss: 0.1267\n",
            "Epoch 20/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2087 - val_loss: 0.1185\n",
            "Epoch 21/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.1115\n",
            "Epoch 22/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1941 - val_loss: 0.1049\n",
            "Epoch 23/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1881 - val_loss: 0.0994\n",
            "Epoch 24/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1762 - val_loss: 0.0952\n",
            "Epoch 25/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1643 - val_loss: 0.0919\n",
            "Epoch 26/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1814 - val_loss: 0.0888\n",
            "Epoch 27/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1622 - val_loss: 0.0862\n",
            "Epoch 28/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1561 - val_loss: 0.0842\n",
            "Epoch 29/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1517 - val_loss: 0.0826\n",
            "Epoch 30/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1454 - val_loss: 0.0815\n",
            "Epoch 31/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1497 - val_loss: 0.0805\n",
            "Epoch 32/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1456 - val_loss: 0.0796\n",
            "Epoch 33/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1479 - val_loss: 0.0787\n",
            "Epoch 34/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1464 - val_loss: 0.0780\n",
            "Epoch 35/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1409 - val_loss: 0.0773\n",
            "Epoch 36/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.0768\n",
            "Epoch 37/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1483 - val_loss: 0.0762\n",
            "Epoch 38/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1446 - val_loss: 0.0757\n",
            "Epoch 39/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.0752\n",
            "Epoch 40/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1342 - val_loss: 0.0748\n",
            "Epoch 41/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1304 - val_loss: 0.0742\n",
            "Epoch 42/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1314 - val_loss: 0.0738\n",
            "Epoch 43/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1273 - val_loss: 0.0734\n",
            "Epoch 44/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1375 - val_loss: 0.0731\n",
            "Epoch 45/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1277 - val_loss: 0.0730\n",
            "Epoch 46/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1247 - val_loss: 0.0728\n",
            "Epoch 47/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1228 - val_loss: 0.0725\n",
            "Epoch 48/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.0724\n",
            "Epoch 49/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1338 - val_loss: 0.0722\n",
            "Epoch 50/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1249 - val_loss: 0.0720\n",
            "Epoch 51/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1183 - val_loss: 0.0718\n",
            "Epoch 52/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1179 - val_loss: 0.0716\n",
            "Epoch 53/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1205 - val_loss: 0.0715\n",
            "Epoch 54/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1194 - val_loss: 0.0713\n",
            "Epoch 55/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1228 - val_loss: 0.0711\n",
            "Epoch 56/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1106 - val_loss: 0.0711\n",
            "Epoch 57/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1158 - val_loss: 0.0711\n",
            "Epoch 58/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1185 - val_loss: 0.0711\n",
            "Epoch 59/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1141 - val_loss: 0.0710\n",
            "Epoch 60/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1170 - val_loss: 0.0710\n",
            "Epoch 61/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1133 - val_loss: 0.0710\n",
            "Epoch 62/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1087 - val_loss: 0.0710\n",
            "Epoch 63/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1163 - val_loss: 0.0710\n",
            "Epoch 64/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1154 - val_loss: 0.0709\n",
            "Epoch 65/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1123 - val_loss: 0.0709\n",
            "Epoch 66/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1065 - val_loss: 0.0708\n",
            "Epoch 67/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.0708\n",
            "Epoch 68/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1155 - val_loss: 0.0709\n",
            "Epoch 69/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1155 - val_loss: 0.0709\n",
            "Epoch 70/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1109 - val_loss: 0.0709\n",
            "Epoch 71/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1152 - val_loss: 0.0710\n",
            "Epoch 72/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 0.0710\n",
            "Epoch 73/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1121 - val_loss: 0.0711\n",
            "Epoch 74/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1100 - val_loss: 0.0712\n",
            "Epoch 75/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0713\n",
            "Epoch 76/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1140 - val_loss: 0.0713\n",
            "Epoch 77/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1089 - val_loss: 0.0714\n",
            "Epoch 78/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1056 - val_loss: 0.0715\n",
            "Epoch 79/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1033 - val_loss: 0.0716\n",
            "Epoch 80/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1037 - val_loss: 0.0719\n",
            "Epoch 81/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1116 - val_loss: 0.0720\n",
            "Epoch 82/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1094 - val_loss: 0.0721\n",
            "Epoch 83/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1072 - val_loss: 0.0723\n",
            "Epoch 84/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1060 - val_loss: 0.0723\n",
            "Epoch 85/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.0725\n",
            "Epoch 86/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0988 - val_loss: 0.0729\n",
            "Epoch 87/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1054 - val_loss: 0.0729\n",
            "Epoch 88/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1035 - val_loss: 0.0732\n",
            "Epoch 89/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.0733\n",
            "Epoch 90/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1035 - val_loss: 0.0734\n",
            "Epoch 91/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0999 - val_loss: 0.0736\n",
            "Epoch 92/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.0735\n",
            "Epoch 93/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1129 - val_loss: 0.0735\n",
            "Epoch 94/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0971 - val_loss: 0.0735\n",
            "Epoch 95/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0993 - val_loss: 0.0736\n",
            "Epoch 96/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1129 - val_loss: 0.0738\n",
            "Epoch 97/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0998 - val_loss: 0.0741\n",
            "Epoch 98/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1075 - val_loss: 0.0743\n",
            "Epoch 99/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0896 - val_loss: 0.0744\n",
            "Epoch 100/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0914 - val_loss: 0.0744\n",
            "Epoch 101/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1051 - val_loss: 0.0744\n",
            "Epoch 102/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0744\n",
            "Epoch 103/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1111 - val_loss: 0.0741\n",
            "Epoch 104/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0982 - val_loss: 0.0743\n",
            "Epoch 105/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1024 - val_loss: 0.0743\n",
            "Epoch 106/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1040 - val_loss: 0.0743\n",
            "Epoch 107/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1017 - val_loss: 0.0744\n",
            "Epoch 108/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.0746\n",
            "Epoch 109/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1037 - val_loss: 0.0746\n",
            "Epoch 110/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1033 - val_loss: 0.0746\n",
            "Epoch 111/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0963 - val_loss: 0.0747\n",
            "Epoch 112/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0990 - val_loss: 0.0748\n",
            "Epoch 113/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0749\n",
            "Epoch 114/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.0748\n",
            "Epoch 115/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1029 - val_loss: 0.0750\n",
            "Epoch 116/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0929 - val_loss: 0.0751\n",
            "Epoch 117/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0920 - val_loss: 0.0752\n",
            "Epoch 118/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.0753\n",
            "Epoch 119/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.0753\n",
            "Epoch 120/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 0.0755\n",
            "Epoch 121/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0988 - val_loss: 0.0756\n",
            "Epoch 122/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.0758\n",
            "Epoch 123/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.0759\n",
            "Epoch 124/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0937 - val_loss: 0.0761\n",
            "Epoch 125/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.0763\n",
            "Epoch 126/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0955 - val_loss: 0.0763\n",
            "Epoch 127/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0937 - val_loss: 0.0762\n",
            "Epoch 128/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1005 - val_loss: 0.0762\n",
            "Epoch 129/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1028 - val_loss: 0.0763\n",
            "Epoch 130/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.0765\n",
            "Epoch 131/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.0766\n",
            "Epoch 132/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 0.0767\n",
            "Epoch 133/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0997 - val_loss: 0.0769\n",
            "Epoch 134/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.0771\n",
            "Epoch 135/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0946 - val_loss: 0.0772\n",
            "Epoch 136/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.0770\n",
            "Epoch 137/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0957 - val_loss: 0.0770\n",
            "Epoch 138/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.0770\n",
            "Epoch 139/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1015 - val_loss: 0.0771\n",
            "Epoch 140/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1032 - val_loss: 0.0773\n",
            "Epoch 141/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.0774\n",
            "Epoch 142/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1030 - val_loss: 0.0774\n",
            "Epoch 143/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.0775\n",
            "Epoch 144/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0945 - val_loss: 0.0775\n",
            "Epoch 145/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.0776\n",
            "Epoch 146/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.0777\n",
            "Epoch 147/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 0.0777\n",
            "Epoch 148/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1006 - val_loss: 0.0777\n",
            "Epoch 149/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0865 - val_loss: 0.0779\n",
            "Epoch 150/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0957 - val_loss: 0.0780\n",
            "Epoch 151/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0781\n",
            "Epoch 152/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.0783\n",
            "Epoch 153/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.0783\n",
            "Epoch 154/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0893 - val_loss: 0.0783\n",
            "Epoch 155/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0974 - val_loss: 0.0783\n",
            "Epoch 156/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.0782\n",
            "Epoch 157/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.0783\n",
            "Epoch 158/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0932 - val_loss: 0.0784\n",
            "Epoch 159/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0932 - val_loss: 0.0784\n",
            "Epoch 160/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0842 - val_loss: 0.0785\n",
            "Epoch 161/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0943 - val_loss: 0.0785\n",
            "Epoch 162/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0932 - val_loss: 0.0786\n",
            "Epoch 163/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0993 - val_loss: 0.0785\n",
            "Epoch 164/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.0787\n",
            "Epoch 165/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.0789\n",
            "Epoch 166/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0928 - val_loss: 0.0790\n",
            "Epoch 167/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 0.0791\n",
            "Epoch 168/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1012 - val_loss: 0.0791\n",
            "Epoch 169/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0906 - val_loss: 0.0791\n",
            "Epoch 170/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0951 - val_loss: 0.0791\n",
            "Epoch 171/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0945 - val_loss: 0.0791\n",
            "Epoch 172/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0987 - val_loss: 0.0791\n",
            "Epoch 173/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0791\n",
            "Epoch 174/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0909 - val_loss: 0.0791\n",
            "Epoch 175/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1004 - val_loss: 0.0792\n",
            "Epoch 176/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0792\n",
            "Epoch 177/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0927 - val_loss: 0.0793\n",
            "Epoch 178/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0909 - val_loss: 0.0793\n",
            "Epoch 179/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0914 - val_loss: 0.0792\n",
            "Epoch 180/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.0791\n",
            "Epoch 181/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0957 - val_loss: 0.0791\n",
            "Epoch 182/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0942 - val_loss: 0.0791\n",
            "Epoch 183/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0921 - val_loss: 0.0792\n",
            "Epoch 184/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0986 - val_loss: 0.0794\n",
            "Epoch 185/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.0796\n",
            "Epoch 186/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0866 - val_loss: 0.0797\n",
            "Epoch 187/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0906 - val_loss: 0.0797\n",
            "Epoch 188/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0867 - val_loss: 0.0796\n",
            "Epoch 189/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0948 - val_loss: 0.0796\n",
            "Epoch 190/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0891 - val_loss: 0.0798\n",
            "Epoch 191/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0914 - val_loss: 0.0799\n",
            "Epoch 192/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0878 - val_loss: 0.0800\n",
            "Epoch 193/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.0801\n",
            "Epoch 194/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0912 - val_loss: 0.0801\n",
            "Epoch 195/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0927 - val_loss: 0.0801\n",
            "Epoch 196/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0922 - val_loss: 0.0801\n",
            "Epoch 197/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0964 - val_loss: 0.0801\n",
            "Epoch 198/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0932 - val_loss: 0.0801\n",
            "Epoch 199/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0979 - val_loss: 0.0801\n",
            "Epoch 200/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0932 - val_loss: 0.0800\n",
            "Epoch 201/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0929 - val_loss: 0.0801\n",
            "Epoch 202/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0912 - val_loss: 0.0801\n",
            "Epoch 203/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0921 - val_loss: 0.0801\n",
            "Epoch 204/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.0802\n",
            "Epoch 205/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.0803\n",
            "Epoch 206/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0977 - val_loss: 0.0805\n",
            "Epoch 207/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0875 - val_loss: 0.0807\n",
            "Epoch 208/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.0808\n",
            "Epoch 209/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0852 - val_loss: 0.0807\n",
            "Epoch 210/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0920 - val_loss: 0.0806\n",
            "Epoch 211/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0952 - val_loss: 0.0807\n",
            "Epoch 212/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0940 - val_loss: 0.0807\n",
            "Epoch 213/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0937 - val_loss: 0.0807\n",
            "Epoch 214/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0807\n",
            "Epoch 215/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0910 - val_loss: 0.0807\n",
            "Epoch 216/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0903 - val_loss: 0.0807\n",
            "Epoch 217/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.0807\n",
            "Epoch 218/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0906 - val_loss: 0.0806\n",
            "Epoch 219/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0950 - val_loss: 0.0805\n",
            "Epoch 220/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0909 - val_loss: 0.0805\n",
            "Epoch 221/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0907 - val_loss: 0.0805\n",
            "Epoch 222/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.0805\n",
            "Epoch 223/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0942 - val_loss: 0.0806\n",
            "Epoch 224/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0953 - val_loss: 0.0807\n",
            "Epoch 225/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.0807\n",
            "Epoch 226/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0982 - val_loss: 0.0807\n",
            "Epoch 227/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0892 - val_loss: 0.0808\n",
            "Epoch 228/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0875 - val_loss: 0.0808\n",
            "Epoch 229/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0904 - val_loss: 0.0807\n",
            "Epoch 230/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0930 - val_loss: 0.0807\n",
            "Epoch 231/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0921 - val_loss: 0.0807\n",
            "Epoch 232/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0910 - val_loss: 0.0807\n",
            "Epoch 233/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0947 - val_loss: 0.0807\n",
            "Epoch 234/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.0807\n",
            "Epoch 235/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0807\n",
            "Epoch 236/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0935 - val_loss: 0.0807\n",
            "Epoch 237/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0926 - val_loss: 0.0807\n",
            "Epoch 238/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0887 - val_loss: 0.0805\n",
            "Epoch 239/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0866 - val_loss: 0.0804\n",
            "Epoch 240/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0887 - val_loss: 0.0804\n",
            "Epoch 241/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0872 - val_loss: 0.0804\n",
            "Epoch 242/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0907 - val_loss: 0.0805\n",
            "Epoch 243/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0884 - val_loss: 0.0805\n",
            "Epoch 244/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0918 - val_loss: 0.0805\n",
            "Epoch 245/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0907 - val_loss: 0.0808\n",
            "Epoch 246/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0901 - val_loss: 0.0809\n",
            "Epoch 247/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0873 - val_loss: 0.0808\n",
            "Epoch 248/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0867 - val_loss: 0.0808\n",
            "Epoch 249/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0903 - val_loss: 0.0808\n",
            "Epoch 250/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0933 - val_loss: 0.0810\n",
            "Epoch 251/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0812 - val_loss: 0.0811\n",
            "Epoch 252/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0897 - val_loss: 0.0812\n",
            "Epoch 253/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.0812\n",
            "Epoch 254/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0963 - val_loss: 0.0813\n",
            "Epoch 255/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0929 - val_loss: 0.0814\n",
            "Epoch 256/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0891 - val_loss: 0.0814\n",
            "Epoch 257/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.0815\n",
            "Epoch 258/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0813\n",
            "Epoch 259/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0831 - val_loss: 0.0812\n",
            "Epoch 260/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0867 - val_loss: 0.0812\n",
            "Epoch 261/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.0812\n",
            "Epoch 262/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 0.0812\n",
            "Epoch 263/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.0812\n",
            "Epoch 264/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0875 - val_loss: 0.0813\n",
            "Epoch 265/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0797 - val_loss: 0.0813\n",
            "Epoch 266/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0815\n",
            "Epoch 267/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0823 - val_loss: 0.0817\n",
            "Epoch 268/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0874 - val_loss: 0.0817\n",
            "Epoch 269/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0845 - val_loss: 0.0816\n",
            "Epoch 270/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.0816\n",
            "Epoch 271/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0882 - val_loss: 0.0815\n",
            "Epoch 272/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0905 - val_loss: 0.0815\n",
            "Epoch 273/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0816\n",
            "Epoch 274/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0887 - val_loss: 0.0816\n",
            "Epoch 275/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0867 - val_loss: 0.0816\n",
            "Epoch 276/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0823 - val_loss: 0.0817\n",
            "Epoch 277/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0855 - val_loss: 0.0818\n",
            "Epoch 278/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0934 - val_loss: 0.0817\n",
            "Epoch 279/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0958 - val_loss: 0.0817\n",
            "Epoch 280/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.0816\n",
            "Epoch 281/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0920 - val_loss: 0.0816\n",
            "Epoch 282/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0912 - val_loss: 0.0818\n",
            "Epoch 283/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0884 - val_loss: 0.0820\n",
            "Epoch 284/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0836 - val_loss: 0.0820\n",
            "Epoch 285/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.0820\n",
            "Epoch 286/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0905 - val_loss: 0.0818\n",
            "Epoch 287/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 0.0817\n",
            "Epoch 288/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0892 - val_loss: 0.0818\n",
            "Epoch 289/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0817\n",
            "Epoch 290/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0858 - val_loss: 0.0816\n",
            "Epoch 291/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0896 - val_loss: 0.0817\n",
            "Epoch 292/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0924 - val_loss: 0.0818\n",
            "Epoch 293/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0919 - val_loss: 0.0818\n",
            "Epoch 294/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0891 - val_loss: 0.0818\n",
            "Epoch 295/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0877 - val_loss: 0.0816\n",
            "Epoch 296/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0907 - val_loss: 0.0815\n",
            "Epoch 297/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0851 - val_loss: 0.0816\n",
            "Epoch 298/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0886 - val_loss: 0.0815\n",
            "Epoch 299/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0856 - val_loss: 0.0814\n",
            "Epoch 300/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0891 - val_loss: 0.0814\n",
            "Epoch 301/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.0814\n",
            "Epoch 302/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0843 - val_loss: 0.0813\n",
            "Epoch 303/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.0813\n",
            "Epoch 304/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0909 - val_loss: 0.0814\n",
            "Epoch 305/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0906 - val_loss: 0.0815\n",
            "Epoch 306/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0856 - val_loss: 0.0816\n",
            "Epoch 307/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0816\n",
            "Epoch 308/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0923 - val_loss: 0.0815\n",
            "Epoch 309/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0816\n",
            "Epoch 310/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.0815\n",
            "Epoch 311/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.0816\n",
            "Epoch 312/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0875 - val_loss: 0.0815\n",
            "Epoch 313/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0872 - val_loss: 0.0817\n",
            "Epoch 314/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.0816\n",
            "Epoch 315/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0892 - val_loss: 0.0817\n",
            "Epoch 316/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0802 - val_loss: 0.0816\n",
            "Epoch 317/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0914 - val_loss: 0.0817\n",
            "Epoch 318/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0892 - val_loss: 0.0816\n",
            "Epoch 319/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0845 - val_loss: 0.0816\n",
            "Epoch 320/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0812 - val_loss: 0.0816\n",
            "Epoch 321/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0817\n",
            "Epoch 322/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0853 - val_loss: 0.0817\n",
            "Epoch 323/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.0818\n",
            "Epoch 324/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.0818\n",
            "Epoch 325/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0877 - val_loss: 0.0818\n",
            "Epoch 326/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0861 - val_loss: 0.0816\n",
            "Epoch 327/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0892 - val_loss: 0.0817\n",
            "Epoch 328/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.0818\n",
            "Epoch 329/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0876 - val_loss: 0.0819\n",
            "Epoch 330/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.0819\n",
            "Epoch 331/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0863 - val_loss: 0.0819\n",
            "Epoch 332/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0873 - val_loss: 0.0818\n",
            "Epoch 333/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 0.0819\n",
            "Epoch 334/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.0819\n",
            "Epoch 335/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0921 - val_loss: 0.0819\n",
            "Epoch 336/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0787 - val_loss: 0.0819\n",
            "Epoch 337/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0862 - val_loss: 0.0819\n",
            "Epoch 338/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.0820\n",
            "Epoch 339/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0845 - val_loss: 0.0819\n",
            "Epoch 340/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0927 - val_loss: 0.0819\n",
            "Epoch 341/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0864 - val_loss: 0.0820\n",
            "Epoch 342/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0871 - val_loss: 0.0821\n",
            "Epoch 343/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.0821\n",
            "Epoch 344/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0915 - val_loss: 0.0819\n",
            "Epoch 345/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0880 - val_loss: 0.0819\n",
            "Epoch 346/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0819\n",
            "Epoch 347/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0856 - val_loss: 0.0818\n",
            "Epoch 348/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0852 - val_loss: 0.0818\n",
            "Epoch 349/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0875 - val_loss: 0.0819\n",
            "Epoch 350/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0898 - val_loss: 0.0819\n",
            "Epoch 351/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.0819\n",
            "Epoch 352/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0887 - val_loss: 0.0822\n",
            "Epoch 353/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0901 - val_loss: 0.0823\n",
            "Epoch 354/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0826 - val_loss: 0.0824\n",
            "Epoch 355/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0841 - val_loss: 0.0825\n",
            "Epoch 356/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.0826\n",
            "Epoch 357/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0826\n",
            "Epoch 358/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0839 - val_loss: 0.0826\n",
            "Epoch 359/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0848 - val_loss: 0.0827\n",
            "Epoch 360/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.0827\n",
            "Epoch 361/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.0825\n",
            "Epoch 362/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0825\n",
            "Epoch 363/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0891 - val_loss: 0.0827\n",
            "Epoch 364/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0897 - val_loss: 0.0828\n",
            "Epoch 365/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0870 - val_loss: 0.0828\n",
            "Epoch 366/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0905 - val_loss: 0.0830\n",
            "Epoch 367/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0851 - val_loss: 0.0829\n",
            "Epoch 368/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.0829\n",
            "Epoch 369/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0876 - val_loss: 0.0829\n",
            "Epoch 370/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0899 - val_loss: 0.0829\n",
            "Epoch 371/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0845 - val_loss: 0.0831\n",
            "Epoch 372/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.0830\n",
            "Epoch 373/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0902 - val_loss: 0.0831\n",
            "Epoch 374/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0873 - val_loss: 0.0830\n",
            "Epoch 375/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0874 - val_loss: 0.0830\n",
            "Epoch 376/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0864 - val_loss: 0.0830\n",
            "Epoch 377/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0890 - val_loss: 0.0829\n",
            "Epoch 378/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.0827\n",
            "Epoch 379/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0857 - val_loss: 0.0826\n",
            "Epoch 380/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0820 - val_loss: 0.0827\n",
            "Epoch 381/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0812 - val_loss: 0.0827\n",
            "Epoch 382/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0848 - val_loss: 0.0826\n",
            "Epoch 383/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0850 - val_loss: 0.0827\n",
            "Epoch 384/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0814 - val_loss: 0.0828\n",
            "Epoch 385/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0885 - val_loss: 0.0829\n",
            "Epoch 386/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.0829\n",
            "Epoch 387/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0875 - val_loss: 0.0829\n",
            "Epoch 388/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0844 - val_loss: 0.0828\n",
            "Epoch 389/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0902 - val_loss: 0.0826\n",
            "Epoch 390/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0826\n",
            "Epoch 391/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0867 - val_loss: 0.0825\n",
            "Epoch 392/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0825\n",
            "Epoch 393/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.0824\n",
            "Epoch 394/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0874 - val_loss: 0.0824\n",
            "Epoch 395/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.0823\n",
            "Epoch 396/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0888 - val_loss: 0.0822\n",
            "Epoch 397/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0823\n",
            "Epoch 398/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.0824\n",
            "Epoch 399/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0875 - val_loss: 0.0825\n",
            "Epoch 400/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.0825\n",
            "Epoch 401/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0827 - val_loss: 0.0825\n",
            "Epoch 402/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0840 - val_loss: 0.0821\n",
            "Epoch 403/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0882 - val_loss: 0.0822\n",
            "Epoch 404/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0889 - val_loss: 0.0824\n",
            "Epoch 405/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0846 - val_loss: 0.0829\n",
            "Epoch 406/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0838 - val_loss: 0.0829\n",
            "Epoch 407/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.0829\n",
            "Epoch 408/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.0829\n",
            "Epoch 409/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0823 - val_loss: 0.0828\n",
            "Epoch 410/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0823 - val_loss: 0.0829\n",
            "Epoch 411/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.0830\n",
            "Epoch 412/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0850 - val_loss: 0.0831\n",
            "Epoch 413/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0877 - val_loss: 0.0832\n",
            "Epoch 414/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0881 - val_loss: 0.0832\n",
            "Epoch 415/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0855 - val_loss: 0.0834\n",
            "Epoch 416/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0835 - val_loss: 0.0835\n",
            "Epoch 417/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0836\n",
            "Epoch 418/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 0.0834\n",
            "Epoch 419/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0856 - val_loss: 0.0834\n",
            "Epoch 420/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0836 - val_loss: 0.0833\n",
            "Epoch 421/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0876 - val_loss: 0.0832\n",
            "Epoch 422/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0804 - val_loss: 0.0830\n",
            "Epoch 423/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0897 - val_loss: 0.0828\n",
            "Epoch 424/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0910 - val_loss: 0.0828\n",
            "Epoch 425/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0845 - val_loss: 0.0828\n",
            "Epoch 426/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0827\n",
            "Epoch 427/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0827\n",
            "Epoch 428/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0821 - val_loss: 0.0828\n",
            "Epoch 429/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0853 - val_loss: 0.0830\n",
            "Epoch 430/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.0830\n",
            "Epoch 431/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0836 - val_loss: 0.0831\n",
            "Epoch 432/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0840 - val_loss: 0.0832\n",
            "Epoch 433/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0828 - val_loss: 0.0830\n",
            "Epoch 434/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0832 - val_loss: 0.0829\n",
            "Epoch 435/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0829\n",
            "Epoch 436/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0829\n",
            "Epoch 437/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0894 - val_loss: 0.0830\n",
            "Epoch 438/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0802 - val_loss: 0.0831\n",
            "Epoch 439/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0833\n",
            "Epoch 440/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.0832\n",
            "Epoch 441/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0829 - val_loss: 0.0833\n",
            "Epoch 442/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0833\n",
            "Epoch 443/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.0832\n",
            "Epoch 444/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0867 - val_loss: 0.0832\n",
            "Epoch 445/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0832\n",
            "Epoch 446/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0852 - val_loss: 0.0831\n",
            "Epoch 447/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0786 - val_loss: 0.0832\n",
            "Epoch 448/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0837 - val_loss: 0.0833\n",
            "Epoch 449/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.0833\n",
            "Epoch 450/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0835\n",
            "Epoch 451/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.0836\n",
            "Epoch 452/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0799 - val_loss: 0.0837\n",
            "Epoch 453/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0852 - val_loss: 0.0838\n",
            "Epoch 454/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0871 - val_loss: 0.0838\n",
            "Epoch 455/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0857 - val_loss: 0.0837\n",
            "Epoch 456/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0878 - val_loss: 0.0839\n",
            "Epoch 457/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.0839\n",
            "Epoch 458/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0875 - val_loss: 0.0839\n",
            "Epoch 459/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0870 - val_loss: 0.0840\n",
            "Epoch 460/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.0839\n",
            "Epoch 461/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0846 - val_loss: 0.0838\n",
            "Epoch 462/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0814 - val_loss: 0.0837\n",
            "Epoch 463/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0829 - val_loss: 0.0837\n",
            "Epoch 464/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.0836\n",
            "Epoch 465/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0794 - val_loss: 0.0835\n",
            "Epoch 466/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0835\n",
            "Epoch 467/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0841 - val_loss: 0.0835\n",
            "Epoch 468/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0803 - val_loss: 0.0836\n",
            "Epoch 469/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0780 - val_loss: 0.0837\n",
            "Epoch 470/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0851 - val_loss: 0.0837\n",
            "Epoch 471/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0874 - val_loss: 0.0838\n",
            "Epoch 472/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0846 - val_loss: 0.0839\n",
            "Epoch 473/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0779 - val_loss: 0.0839\n",
            "Epoch 474/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.0837\n",
            "Epoch 475/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0853 - val_loss: 0.0836\n",
            "Epoch 476/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.0836\n",
            "Epoch 477/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0814 - val_loss: 0.0836\n",
            "Epoch 478/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.0836\n",
            "Epoch 479/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0835\n",
            "Epoch 480/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0826 - val_loss: 0.0834\n",
            "Epoch 481/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0835 - val_loss: 0.0834\n",
            "Epoch 482/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0803 - val_loss: 0.0835\n",
            "Epoch 483/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0791 - val_loss: 0.0835\n",
            "Epoch 484/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0834\n",
            "Epoch 485/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0770 - val_loss: 0.0835\n",
            "Epoch 486/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0849 - val_loss: 0.0835\n",
            "Epoch 487/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0853 - val_loss: 0.0835\n",
            "Epoch 488/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0847 - val_loss: 0.0834\n",
            "Epoch 489/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0835\n",
            "Epoch 490/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0834 - val_loss: 0.0833\n",
            "Epoch 491/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0833 - val_loss: 0.0834\n",
            "Epoch 492/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0834\n",
            "Epoch 493/500\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0786 - val_loss: 0.0835\n",
            "Epoch 494/500\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0803 - val_loss: 0.0836\n",
            "Epoch 495/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0838 - val_loss: 0.0837\n",
            "Epoch 496/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0836\n",
            "Epoch 497/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0852 - val_loss: 0.0834\n",
            "Epoch 498/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0794 - val_loss: 0.0832\n",
            "Epoch 499/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0854 - val_loss: 0.0831\n",
            "Epoch 500/500\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0885 - val_loss: 0.0831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_9WSdKgR8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c834eca-06b5-4960-a62f-68a4014ed00d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 128)               76928     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 79,569\n",
            "Trainable params: 79,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOMTnQxmT6Jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c963ce0e-f3aa-4fa9-f869-05c847e0c057"
      },
      "source": [
        "# evaluate the keras model\n",
        "error = model.evaluate(x=test_features , y = test_labels)\n",
        "print('Error: %.2f'% (error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0818\n",
            "Error: 0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRIExVQVYUGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5e639360-56b9-4cce-de73-016bb844c29d"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['training', 'validation'])\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ov_WrlUyhg"
      },
      "source": [
        "#for layer in model.layers: \n",
        "#    if len(layer.get_weights()) > 0: \n",
        "#        print(layer.name, layer.get_weights()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "dKm_LHThr1cF",
        "outputId": "a4f9f746-5868-43ad-e0a9-2782bcd9454c"
      },
      "source": [
        "class MyTuner(kt.RandomSearch):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "    # You can add additional HyperParameters for preprocessing and custom training loops\n",
        "    # via overriding `run_trial`\n",
        "\n",
        "    super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
        "\n",
        "# Uses same arguments as the BayesianOptimization Tuner.\n",
        "tuner = MyTuner(nn_supercapacitor,\n",
        "                objective='val_loss',\n",
        "                max_trials=1000,)\n",
        "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
        "\n",
        "tuner.search(\n",
        "    train_features, train_labels,\n",
        "        validation_split=0.2,\n",
        "            epochs=1000,\n",
        "            callbacks=[\n",
        "              tf.keras.callbacks.EarlyStopping(\n",
        "              monitor='val_loss',\n",
        "              patience=100,\n",
        "              )\n",
        "              ]\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "Dense Layers      |8                 |?                 \n",
            "Dense_0_layer     |16                |?                 \n",
            "learning_rate     |1e-05             |?                 \n",
            "\n",
            "Epoch 1/1000\n",
            "12/12 [==============================] - 1s 28ms/step - loss: 0.3695 - val_loss: 0.3042\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3675 - val_loss: 0.3025\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3655 - val_loss: 0.3007\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3634 - val_loss: 0.2990\n",
            "Epoch 5/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3949"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-155d0ac59823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               tf.keras.callbacks.EarlyStopping(\n\u001b[1;32m     20\u001b[0m               \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m               \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m               )\n\u001b[1;32m     23\u001b[0m               ]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-155d0ac59823>\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# via overriding `run_trial`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyTuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Uses same arguments as the BayesianOptimization Tuner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}